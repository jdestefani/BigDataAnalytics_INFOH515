{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INFO-H515 - Big Data Scalable Analytics](https://uv.ulb.ac.be/course/view.php?id=85246?username=guest)\n",
    "\n",
    "\n",
    "## TP 1 - Introduction to Spark ML library: Churn prediction\n",
    "\n",
    "*Notebook content adapted from https://github.com/bensadeghi/pyspark-churn-prediction*\n",
    "\n",
    "*Materials orignally developed by Yann-AÃ«l Le Borgne, Jacopo De Stefani and Gianluca Bontempi*\n",
    "\n",
    "#### *Jacopo De Stefani, Theo Verhelst and Gianluca Bontempi*\n",
    "\n",
    "####  01/04/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class aims at providing an overview of the Spark framework for loading a dataset, performing some data exploration, and building prediction models using the [Spark MLlib (machine learning) library](https://spark.apache.org/docs/latest/ml-guide.html). \n",
    "\n",
    "Take a moment to check the content of the Spark MLlib library.\n",
    "\n",
    "The case study is a churn prediction problem, which consists in minimizing customer defection by predicting which customers are likely to cancel a subscription to a service. Churn prediction is a crucial problem for many businesses. Originally used within the telecommunications industry, it has become common practice across banks, ISPs, insurance firms, and other verticals. Churn prediction is essentially a classification problem : Will a given customer quit or not?\n",
    "\n",
    "Example of data that we will use:\n",
    "\n",
    "![](./img/ExampleDataChurn.png)\n",
    "\n",
    "\n",
    "### Class objectives:\n",
    "\n",
    "* Subsample a Spark Dataframe for local visual/statistical exploration using Python Pandas\n",
    "* Train a prediction model using Spark ML library\n",
    "* Use pipelines to prepare datasets, and run cross validation\n",
    "* Assess model performances using different accuracy metrics\n",
    "* Make predictions on new data (not used during training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:46:41.571555Z",
     "start_time": "2020-03-02T13:46:40.894333Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "# 1) Data loading and preliminary data exploration \n",
    "\n",
    "Let us first start a Spark session, load a dataset, and perform some preliminary data exploration using the Spark SQL and DataFrames modules. Additional examples of functions and uses of these modules can be found [here](https://spark.apache.org/docs/latest/sql-programming-guide.html). \n",
    "\n",
    "#### Start Spark session\n",
    "\n",
    "A Spark session is created by using the pyspark.sql.SparkSession object. See [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sparksession) for the API documentation on the SparkSession Object. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T13:46:43.902283Z",
     "start_time": "2020-03-02T13:46:43.778635Z"
    },
    "solution": "shown"
   },
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Start Spark session with local master and 2 cores, and name it 'churnPrediction'\n",
    "Use\n",
    "```\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "...TOFILL\n",
    "    .getOrCreate()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now connect to the Spark user interface on port 4040 (IP:4040 in your browser).\n",
    "\n",
    "![](./img/spark_init.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Use `spark.stop()` to stop the session.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching and Importing Churn Data\n",
    "\n",
    "For this class, we will be using the [Orange Telecoms Churn Dataset](http://www.kdd.org/kdd-cup/view/kdd-cup-2009). It consists of cleaned customer activity data (features), along with a churn label specifying whether the customer canceled their subscription or not. The data can be fetched from BigML's S3 bucket, [churn-80](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv) and [churn-20](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv). The two sets are from the same batch, but have been split by an 80/20 ratio. We'll use the larger set for training and cross-validation purposes, and the smaller set for final testing and model performance evaluation. The two data sets have been included in this repository for convenience.\n",
    "\n",
    "Let us load the two CSV data sets into DataFrames, keeping the header information and caching them into memory for quick, repeated access. We will also print the schema of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the ./data/churn-bigml-80.csv and store in CV_data\n",
    "CV_data = spark.read.load('./data/churn-bigml-80.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Spark UI. Notice there are two jobs for loading the data. The first one was to infer the column types (only 2 records read). The second actually loads the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Load the ./data/churn-bigml-20.csv and store in a variable final_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Cache CV_data, and print its schema (use `cache` and `printSchema` functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data.cache()\n",
    "CV_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Count the number of observsations in CV_data (use `count` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the Spark UI that data was properly cached. See the storage tab. \n",
    "\n",
    "Let us take the first five rows of CV_data to see what they look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use Pandas on top of Spark _take()_ function to get a prettier print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_data.take(50)[15:25], columns=CV_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "\n",
    "Spark DataFrames include some [built-in functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) for statistical processing. The _describe()_ function performs summary statistics calculations on all numeric columns, and returns them as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data.describe().toPandas().set_index('summary').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations and Data Preparation\n",
    "\n",
    "We can also perform our own statistical analyses, using the [ML statistics package](http://spark.apache.org/docs/2.2.0/ml-statistics.html) or other python packages. Here, we're use the Pandas library to examine correlations between the numeric columns by generating scatter plots of them.\n",
    "\n",
    "For the Pandas workload, we don't want to pull the entire data set into the Spark driver, as that might exhaust the available RAM and throw an out-of-memory exception. Instead, we randomly sample a portion of the data (say 10%) to get a rough idea of how it looks. \n",
    "\n",
    "Let us first select the numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in CV_data.dtypes if t[1] == 'int' or t[1] == 'double']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "* Select the numeric features from the CV_data dataframe (`select` function)\n",
    "* Sample 10% of the dataframe without replacement (`sample` function)\n",
    "* Convert the resulting Spark dataframe to a Panda dataframe (`toPandas` function), and store in a variable `sampled_data``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let us plot the correlations between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = pd.plotting.scatter_matrix(sampled_data, figsize=(12, 12));\n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that there are several highly correlated fields, ie _Total day minutes_ and _Total day charge_. Such correlated data will not be very beneficial for our model training runs, so we are going to remove them. We will do so by dropping one column of each pair of correlated fields, along with the _State_ and _Area code_ columns. Dropping a column is done by using the `drop` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Remove the _State_ , _Area code_ , _Total day charge_ , _Total eve charge_ , _Total night charge_ and _Total intl charge_ variables from both CV_data and final_test_data dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are in the process of manipulating the data sets, let us transform the categorical data into numeric as required by the machine learning routines. Transforming columns in Spark is done using the [`withColumn`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) function, taking as parameter the name of the new column (or an existing one for transforming the content of an existing column), and the function used to generate the content of the new column.\n",
    "\n",
    "The function used to generate the content of the new column must be a user-defined function (http://changhsinlee.com/pyspark-udf/). For example, to convert a column that contains Yes/No to 1 and 0, let us first create a function that makes this transformation, using a simple user-defined functions `transform1` that maps Yes/True and No/False to 1 and 0, respectively. \n",
    "\n",
    "Creating a new column is done by using the \n",
    "\n",
    "* Import `udf` from [pyspark.sql.function](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "* Write the user defined function `transform1`, and make it a `udf`\n",
    "* Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms 'Yes' and 'No' to '1.0' or '0.0'\n",
    "def transform1(value):\n",
    "    result=-1.0\n",
    "    if value=='Yes':\n",
    "        result=1.0\n",
    "    if value=='No':\n",
    "        result=0.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make it a user-defined function (udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "transform1_udf = udf(lambda k: transform1(k), DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally transform the column using the `withColumn' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data = CV_data.withColumn('International plan', transform1_udf(CV_data['International plan'])) \\\n",
    "    .withColumn('Voice mail plan', transform1_udf(CV_data['Voice mail plan'])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also do the transformation for `final_test_data`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = final_test_data\\\n",
    "    .withColumn('International plan', transform1_udf(final_test_data['International plan']))\\\n",
    "    .withColumn('Voice mail plan', transform1_udf(final_test_data['Voice mail plan'])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Transform the _Churn_ column (boolean) in a numeric feature (0.0 for False, 1.0 for True), by adapting the code above with two new functions `transform2` and `transform2_udf`, for both `CV_data` and `final_test_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "Let us take a quick look at the resulting data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_data.take(5), columns=CV_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Modelling using the Spark MLlib Package (RDD-based)\n",
    "\n",
    "The RDD-based [MLlib package](http://spark.apache.org/docs/latest/mllib-guide.html) provides a variety of machine learning algorithms for classification, regression, cluster and dimensionality reduction, as well as utilities for model evaluation. The decision tree is a popular classification algorithm, and we'll be using it below.  \n",
    "\n",
    "### Decision Tree Models\n",
    "\n",
    "Decision trees have played a significant role in data mining and machine learning since the 1960's. They generate white-box classification and regression models which can be used for feature selection and sample prediction. The transparency of these models is a big advantage over black-box learners, because the models are easy to understand and interpret, and they can be readily extracted and implemented in any programming language (with nested if-else statements) for use in production environments. Furthermore, decision trees require almost no data preparation (ie normalization) and can handle both categorical and continuous data. To remedy over-fitting and improve prediction accuracy, decision trees can also be limited to a certain depth or complexity, or bundled into ensembles of trees (ie random forests).\n",
    "\n",
    "A decision tree is a predictive model which maps observations (features) about an item to conclusions about the item's label or class. The model is generated using a top-down approach, where the source dataset is split into subsets using a statistical measure, often in the form of the Gini index or information gain via Shannon entropy. This process is applied recursively until a subset contains only samples with the same target class, or is halted by a predefined stopping criteria.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "RDD-based MLlib classifiers and regressors require data sets in a format of rows of type _LabeledPoint_, which separates row labels and feature lists, and names them accordingly. The custom _labelData()_ function shown below performs the row parsing. We'll pass it the prepared data set (CV_data) and split it further into training and testing sets. A [decision tree classifier model](https://spark.apache.org/docs/2.1.1/mllib-decision-tree.html) is then generated using the training data, using a maxDepth of 2, to build a \"shallow\" tree. The tree depth can be regarded as an indicator of model complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "def labelData(data):\n",
    "    # label: row[end], features: row[0:end-1]\n",
    "    return data.map(lambda row: LabeledPoint(row[-1], row[:-1]))\n",
    "\n",
    "training_data, testing_data = labelData(CV_data.rdd).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = DecisionTree.trainClassifier(training_data, numClasses=2, maxDepth=2,\n",
    "                                     categoricalFeaturesInfo={1:2, 2:2},\n",
    "                                     impurity='gini', maxBins=32)\n",
    "\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _toDebugString()_ function provides a print of the tree's decision nodes and final prediction outcomes at the end leafs. We can see that features 12 and 4 are used for decision making and should thus be considered as having high predictive power to determine a customer's likeliness to churn. It's not surprising that these feature numbers map to the fields _Customer service calls_ and _Total day minutes_ . Decision trees are often used for feature selection because they provide an automated mechanism for determining the most important features (those closest to the tree root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature 12:', CV_data.columns[12])\n",
    "print('Feature 4: ', CV_data.columns[4])\n",
    "print('Feature 2: ', CV_data.columns[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the structure of a Labeled Point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = training_data.take(1)[0]\n",
    "print(datapoint.features) \n",
    "print(datapoint.label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Predictions of the testing data's churn outcome are made with the model's _predict()_ function and grouped together with the actual churn label of each customer data using _getPredictionsLabels()_.\n",
    "\n",
    "We'll use MLlib's _MulticlassMetrics_ (https://spark.apache.org/docs/2.1.1/mllib-evaluation-metrics.html#binary-classification) for the model evaluation, which takes rows of (prediction, label) tuples as input. It provides metrics such as precision, recall, F1 score and confusion matrix, which have been bundled for printing with the custom _printMetrics()_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def getPredictionsLabels(model, test_data):\n",
    "    predictions = model.predict(test_data.map(lambda r: r.features))\n",
    "    return predictions.zip(test_data.map(lambda r: r.label))\n",
    "\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    print('Precision of True '+ str(metrics.precision(1)))\n",
    "    print('Precision of False'+ str(metrics.precision(0)))\n",
    "    print('Recall of True    '+ str(metrics.recall(1)))\n",
    "    print('Recall of False   '+ str(metrics.recall(0)))\n",
    "    print('F-1 Score         '+ str(metrics.fMeasure(1.0)))\n",
    "    print('Confusion Matrix\\n'+ str(metrics.confusionMatrix().toArray()))\n",
    "\n",
    "predictions_and_labels = getPredictionsLabels(model, testing_data)\n",
    "\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy, ie F-1 score, seems quite good, but one troubling issue is the discrepancy between the recall measures. The recall (aka sensitivity) for the Churn=False samples is high, while the recall for the Churn=True examples is relatively low. Business decisions made using these predictions will be used to retain the customers most likely to leave, not those who are likely to stay. Thus, we need to ensure that our model is sensitive to the Churn=True samples.\n",
    "\n",
    "Perhaps the model's sensitivity bias toward Churn=False samples is due to a skewed distribution of the two types of samples. Let's try grouping the CV_data DataFrame by the _Churn_ field and counting the number of instances in each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_data.groupby('Churn').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "\n",
    "There are roughly 6 times as many False churn samples as True churn samples. We can put the two sample types on the same footing using stratified sampling. The DataFrames _sampleBy()_ function does this when provided with fractions of each sample type to be returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Keep all instances of the Churn=True class, but downsampling the Churn=False class to a fraction of 388/2278. Put the resulting dataframe in a variable `stratified_CV_data`. Use the `sampleBy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Check the number of examples per class is balanced (use the `groupBy`/`count` as above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Build a new model using the evenly distributed data set and see how it performs (adapt the code above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution\n",
    "## ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With these new recall values, we can see that the stratified data was helpful in building a less biased model, which will ultimately provide more generalized and robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Modelling using the Spark ML Package (DataFrame based)\n",
    "\n",
    "The Dataframe-based [MLlib package](http://spark.apache.org/docs/latest/ml-guide.html) is the newer library of machine learning routines. It provides an API for pipelining data transformers, estimators and model selectors. We'll use it here to perform cross-validation across several decision trees with various _maxDepth_ parameters in order to find the optimal model.\n",
    "\n",
    "### Pipelining\n",
    "\n",
    "The ML package needs data be put in a (label: Double, features: Vector) DataFrame format with correspondingly named fields. The _vectorizeData()_ function below performs this formatting.\n",
    "\n",
    "Next we'll pass the data through a pipeline of two transformers, _StringIndexer()_ and _VectorIndexer()_ which index the label and features fields respectively. Indexing categorical features allows decision trees to treat categorical features appropriately, improving performance. The final element in our pipeline is an estimator (a decision tree classifier) training on the indexed labels and features.\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Given the data set at hand, we would like to determine which parameter values of the decision tree produce the best model. We need a systematic approach to quantitatively measure the performance of the models and ensure that the results are reliable. This task of model selection is often done using cross validation techniques. A common technique is k-fold cross validation, where the data is randomly split into k partitions. Each partition is used once as the testing data set, while the rest are used for training. Models are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements. The average of the performance scores is often taken to be the overall score of the model, given its build parameters.\n",
    "\n",
    "For model selection we can search through the model parameters, comparing their cross validation performances. The model parameters leading to the highest performance metric produce the best model.\n",
    "\n",
    "The ML package supports k-fold cross validation, which can be readily coupled with a parameter grid builder and an evaluator to construct a model selection workflow. Below, we'll use a transformation/estimation pipeline to train our models. The cross validator will use the _ParamGridBuilder_ to iterate through the _maxDepth_ parameter of the decision tree and evaluate the models using the F1-score, repeating 3 times per parameter value for reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def vectorizeData(data):\n",
    "    return data.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).toDF(['label','features'])\n",
    "\n",
    "vectorized_CV_data = vectorizeData(stratified_CV_data.rdd)\n",
    "\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(vectorized_CV_data)\n",
    "\n",
    "# Automatically identify categorical features and index them\n",
    "featureIndexer = VectorIndexer(inputCol='features',\n",
    "                               outputCol='indexedFeatures',\n",
    "                               maxCategories=2).fit(vectorized_CV_data)\n",
    "\n",
    "# Train a DecisionTree model\n",
    "dTree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree])\n",
    "\n",
    "# Search through decision tree's maxDepth parameter for best model\n",
    "paramGrid = ParamGridBuilder().addGrid(dTree.maxDepth, [2,3,4,5,6,7]).build()\n",
    "\n",
    "# Set F-1 score as evaluation metric for best model selection\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',\n",
    "                                              predictionCol='prediction', metricName='f1')    \n",
    "\n",
    "# Set up 3-fold cross validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "CV_model = crossval.fit(vectorized_CV_data)\n",
    "\n",
    "# Fetch best model\n",
    "tree_model = CV_model.bestModel.stages[2]\n",
    "print(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the best tree model produced using the cross-validation process is one with a depth of 5. So we can assume that our initial \"shallow\" tree of depth 2 in the previous section was not complex enough, while trees of depth higher than 5 overfit the data and will not perform well in practice.\n",
    "\n",
    "### Predictions and Model Evaluation\n",
    "\n",
    "The actual performance of the model can be determined using the final_test_data set which has not been used for any training or cross-validation activities. We'll transform the test set with the model pipeline, which will map the labels and features according to the same recipe. The evaluator will provide us with the F-1 score of the predictions, and then we'll print them along with their probabilities. Predictions on new, unlabeled customer activity data can also be made using the same pipeline CV_model._transform()_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test_data = vectorizeData(final_test_data.rdd)\n",
    "\n",
    "transformed_data = CV_model.transform(vectorized_test_data)\n",
    "print(evaluator.getMetricName(), 'accuracy:', evaluator.evaluate(transformed_data))\n",
    "\n",
    "predictions = transformed_data.select('indexedLabel', 'prediction', 'probability')\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction probabilities can be very useful in ranking customers by their likeliness to defect. This way, the limited resources available to the business for retention can be focused on the appropriate customers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant links\n",
    "\n",
    "* Spark SQL API: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "* Spark DataFrame API: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "* Spark ML libraries: http://spark.apache.org/docs/latest/ml-guide.html\n",
    "* User-defined functions: http://changhsinlee.com/pyspark-udf/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "* Notebok adapted from https://github.com/bensadeghi/pyspark-churn-prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
